\section{Introduction}

Linear Discriminant Analysis (LDA) is a foundational technique in statistical pattern recognition and machine learning, widely used for both classification and dimensionality reduction tasks. Introduced by R. A. Fisher, LDA aims to find a linear combination of features that best separates multiple classes of objects or events. By projecting high-dimensional data onto a lower-dimensional space, LDA enhances class separability, making it an essential tool for preprocessing and data analysis in various fields.

In the context of classification, LDA assumes that different classes generate data based on Gaussian distributions with a common covariance matrix but different mean vectors. This assumption leads to linear decision boundaries that are easy to compute and interpret. LDA is particularly effective when the statistical properties of the classes are well understood, and it has been successfully applied in areas such as image recognition, bioinformatics, and speech recognition.

For dimensionality reduction, LDA seeks to reduce the number of features while preserving as much of the class discriminatory information as possible. This is achieved by projecting the data onto a lower-dimensional subspace where the between-class variance is maximized, and the within-class variance is minimized. This property makes LDA a valuable method for overcoming the curse of dimensionality and improving the performance of machine learning algorithms on high-dimensional datasets.

This article explores the theoretical foundations of LDA, starting from the principles of classification theory and the concept of the Bayes optimal classifier. We discuss how LDA approximates the Bayes classifier under certain assumptions and derive the LDA classifier using estimates from training data. Additionally, we delve into the role of LDA in dimensionality reduction, formulating the optimization problem that balances between-class and within-class scatter and discussing solutions for both single and multiple projection dimensions.

The exposition builds upon established works in statistical learning and pattern recognition. Notable references include Hastie, Tibshirani, and Friedman's \emph{The Elements of Statistical Learning}~\cite{hastie2009elements}, which provides comprehensive coverage of statistical learning techniques, and Bishop's \emph{Pattern Recognition and Machine Learning}~\cite{bishop2006pattern}, offering in-depth insights into machine learning algorithms. Other foundational texts such as Fukunaga's \emph{Introduction to Statistical Pattern Recognition}~\cite{fukunaga1990introduction} and Duda et al.'s \emph{Pattern Classification}~\cite{duda2012pattern} have also informed the development of this article.
