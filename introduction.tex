\section{Introducción}

El Análisis Discriminante Lineal (LDA) es una técnica fundamental en el reconocimiento de patrones y aprendizaje automático que cumple un doble propósito: actúa como clasificador y como método de reducción de dimensionalidad. Desarrollado originalmente por R. A. Fisher, LDA encuentra combinaciones lineales de características que optimizan la separación entre clases, lo que lo hace valioso tanto para el análisis como para el preprocesamiento de datos de alta dimensionalidad.

Como clasificador, LDA opera bajo el supuesto de que las clases siguen distribuciones gaussianas con matrices de covarianza iguales pero medias diferentes. Esto conduce a fronteras de decisión lineales, eficientes e interpretables que son efectivas en diversas aplicaciones, desde el reconocimiento de imágenes hasta la bioinformática.

En su rol de reducción de dimensionalidad, LDA proyecta los datos en un espacio de menor dimensión mientras preserva la información discriminatoria de las clases. Al optimizar la razón entre la varianza entre clases y la varianza dentro de las clases, proporciona reducción de dimensionalidad manteniendo el rendimiento de clasificación.

Este artículo presenta los fundamentos teóricos de LDA, comenzando con teoría de clasificación y el clasificador óptimo de Bayes. Mostramos cómo LDA aproxima el clasificador de Bayes y derivamos su formulación utilizando datos de entrenamiento. También examinamos sus aspectos de reducción de dimensionalidad, analizando casos de proyección tanto simples como múltiples.

El trabajo se basa en textos clásicos de aprendizaje estadístico, incluyendo \emph{The Elements of Statistical Learning}~\cite{hastie2009elements}, \emph{Pattern Recognition and Machine Learning}~\cite{bishop2006pattern}, \emph{Introduction to Statistical Pattern Recognition}~\cite{fukunaga1990introduction} y \emph{Pattern Classification}~\cite{duda2012pattern}.
