\begin{abstract}
Este artículo estudia el Análisis Discriminante Lineal (LDA) y sus aplicaciones en clasificación y reducción de dimensionalidad. Comenzamos presentando los fundamentos de la clasificación, donde los clasificadores asignan vectores de características a etiquetas de clase, y analizamos cómo el clasificador óptimo de Bayes minimiza el error de predicción mediante el uso de probabilidades a posteriori.

Dado que en la práctica raramente se dispone de las probabilidades a posteriori exactas, mostramos cómo LDA aproxima el clasificador de Bayes bajo el supuesto de densidades condicionales gaussianas con matrices de covarianza iguales. Explicamos el proceso de estimación de estos parámetros a partir de datos de entrenamiento para construir funciones discriminantes prácticas.

Posteriormente, examinamos el rol de LDA en la reducción de dimensionalidad, donde se proyectan los datos a un espacio de menor dimensión maximizando la separación entre clases. Esto conlleva resolver un problema de optimización que equilibra la varianza entre clases y dentro de las clases mediante descomposición en valores propios.

El artículo incluye tanto las derivaciones teóricas como un algoritmo computacionalmente eficiente para implementar LDA.
\end{abstract}
