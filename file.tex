\documentclass[a4paper,12pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-nodecimaldot]{babel} % con puntos en vez de comas en los decimales
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks=true]{hyperref}
\usepackage{color,listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left, %none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=false,
  breakatwhitespace=true,
  tabsize=3,
  backgroundcolor=\color{backcolour},
  keepspaces=true
}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\begin{document}

\section{LDA as classifier}


Classification is one of the fundamental tasks in machine learning and statistical analysis. At its core, a classifier is a function that takes input features and assigns them to predefined categories or classes. The goal is to develop models that can make accurate predictions on new, unseen data by learning patterns from training examples. While there are many approaches to classification, from simple rule-based systems to complex neural networks, we're particularly interested in understanding how well a classifier performs and what makes a classifier optimal. This leads us to a formal framework for evaluating classifier performance through the concept of prediction error.

Our approach to developing an effective classifier will follow a principled path: we'll first establish the theoretical foundations using probability theory to understand what makes a classifier optimal. This will lead us to the concept of posterior probabilities and their role in classification decisions. Then, we'll bridge the gap between theory and practice by showing how these theoretical insights can be implemented using training data and linear algebra techniques, ultimately arriving at Linear Discriminant Analysis (LDA) as a practical and powerful classification method.

\subsection{Theory of Classification}

\begin{definition}
Let $\mathcal{X} \subseteq \mathbb{R}^p$ be the feature space and $\mathcal{Y} = \{1,\ldots,K\}$ be the set of possible class labels. A classifier is a function $\hat{G}: \mathcal{X} \to \mathcal{Y}$ that assigns a class label to each point in the feature space.
\end{definition}

The goal of classification is to find a classifier that accurately predicts the true class label $Y \in \mathcal{Y}$ of new observations $X \in \mathcal{X}$. To evaluate and compare different classifiers, we need a formal way to measure their performance. This leads us to the concept of prediction error.

\begin{definition}
Let $X \in \mathbb{R}^p$ be a random vector of features, $Y \in \{1,\ldots,K\}$ be a categorical random variable representing the class label, and $\hat{G}: \mathbb{R}^p \to \{1,\ldots,K\}$ be a classifier function. Let $L: \{1,\ldots,K\} \times \{1,\ldots,K\} \to \mathbb{R}$ be a loss function that measures the cost of misclassification.

    The Expected Prediction Error (EPE) of the classifier $\hat{G}(X)$ is defined as the expected value of the loss function $L(Y, \hat{G}(X))$ over the joint distribution of $X$ and $Y$:
\[
\text{EPE}(\hat{G}) = \mathbb{E}[L(Y, \hat{G}(X))]
\]
\end{definition}

The choice of loss function is crucial in measuring the performance of a classifier. Different loss functions can lead to different optimal classifiers. In this section, we'll focus on the zero-one loss function, which assigns a penalty of $1$ for every misclassification and $0$ for correct classification.

\begin{definition}
The zero-one loss function is defined as:
\[
L(y, k) = \begin{cases}
0 & \text{if } y = k \\
1 & \text{if } y \neq k
\end{cases}
\]
\end{definition}

We are interested in finding the classifier that minimizes the expected prediction error under the zero-one loss function, which, intuitively, is the classifier that assigns each input to the class with the highest posterior probability.

\begin{theorem}
Let $X$ be a random vector with probability density function $f_X: \mathbb{R}^p \to \mathbb{R}_{\geq 0}$, and let $Y$ be a discrete random variable taking values in $\{1,\ldots,K\}$. Assume that the conditional probability density functions $f_{X|Y}(x|y)$ exist for all $y \in \{1,\ldots,K\}$.

The classifier that minimizes the expected prediction error (EPE) under the zero-one loss function assigns each input $x$ to the class with the highest posterior probability $\mathbb{P}(Y = k \mid X = x)$.
\[
\hat{G}(x) = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid X = x)
\]
\end{theorem}

\begin{proof}
The expected prediction error of the classifier $\hat{G}(X)$ can be written as:
\begin{align*}
\mathbb{E}[L(Y, \hat{G}(X))] & = \sum_{y=1}^K \int_{\mathbb{R}^{p}} L(y, \hat{G}(x)) f_{X|Y}(x|y)\mathbb{P}(Y=y)\, dx \\
& = \int_{\mathbb{R}^{p}} \left( \sum_{y=1}^K L(y, \hat{G}(x)) \mathbb{P}(Y = y \mid X = x) \right) f_X(x) \, dx \\
& = \mathbb{E}_X\left[ \sum_{y=1}^K L(y, \hat{G}(X)) \mathbb{P}(Y = y \mid X = X) \right]
\end{align*}
where we used Bayes' theorem and the law of total probability. To minimize this expectation, it suffices to minimize the inner sum for each $x$:
\[
\hat{G}(x) = \arg\min_{k=1}^K \sum_{y=1}^K L(y, k) \mathbb{P}(Y = y \mid X = x)
\]
Using the zero-one loss function:
\begin{align*}
\hat{G}(x) & = \arg\min_{k=1}^K \left(1 - \mathbb{P}(Y = k \mid X = x)\right) \\
& = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid X = x)
\end{align*}
\end{proof}

This theoretical result is known as the Bayes optimal classifier, and it provides the foundation for developing practical classification algorithms.

\begin{definition}
The Bayes optimal classifier is the classifier that assigns each input $x$ to the class $k$ that maximizes the posterior probability $\mathbb{P}(Y = k \mid X = x)$:
\[
\hat{G}(x) = \arg\max_{k=1}^K \mathbb{P}(Y = k \mid X = x)
\]
\end{definition}
The Bayes optimal classifier minimizes the expected prediction error and provides the best possible classification performance given the true class-conditional distributions.

\subsection{LDA: Approximating the Bayes optimal classifier}

In practice, obtaining the Bayes optimal classifier is often impossible because it requires knowledge of the posterior probabilities $\mathbb{P}(Y = k \mid X = x)$. These posterior probabilities are typically unknown, as they depend on the underlying distribution of the data, which is rarely available in real-world scenarios. Therefore, we need to approximate the Bayes classifier by making assumptions and using estimation techniques.

To approximate the Bayes classifier, we will employ Bayes' theorem. We introduce the following notation:
\begin{itemize}
    \item Let $f_k(x)$ represent the class-conditional density of $X$ for class $Y = k$.
    \item Let $\pi_k$ represent the prior probability of class $k$.
\end{itemize}

Using Bayes' theorem, we can express the posterior probability $\mathbb{P}(Y = k \mid X = x)$ as:
\[
\mathbb{P}(Y = k \mid X = x) = \frac{f_k(x) \, \pi_k}{\sum_{\ell=1}^K f_\ell(x) \, \pi_\ell}
\]

This formulation allows us to approximate the posterior probabilities using estimates of the class-conditional densities $f_k(x)$ and the priors $\pi_k$.

Additionally, LDA makes the following assumptions:

\begin{enumerate}
    \item \textbf{Multivariate Gaussian Distributions}: The class-conditional densities $f_k(x)$ follow multivariate Gaussian distributions with mean vector $\mu_k$ and common covariance matrix $\Sigma$:
    \[
    f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\right)
    \]
    \item \textbf{Equal Covariance Matrix}: The covariance matrix $\Sigma$ is the same for all classes.
\end{enumerate}

The parameters of the distribution and the priors can be estimated from training data. With this purpose, we define some notation:

\begin{itemize}
    \item Let $N_k$ be the number of observations in class $k$.
    \item Let $N = \sum_{k=1}^K N_k$ be the total number of observations.
    \item Let $\{(x_i, g_i)\}_{i=1}^N$ be the training data, where $x_i$ are the feature vectors and $g_i$ are the true class labels.
\end{itemize}

We will estimate the priors $\pi_k$ by the class proportions, and the class means and covariance matrix by the sample means and unbiased sample covariance matrix of the training data:

\begin{itemize}
    \item \textbf{Class Priors}:
    \[
    \hat{\pi}_k = \frac{N_k}{N}
    \]
    \item \textbf{Class Means}:
    \[
    \hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i = k} x_i
    \]
    \item \textbf{Covariance Matrix}:
    \[
    \hat{\Sigma} = \frac{1}{N - K} \sum_{k=1}^K \sum_{i: g_i = k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \label{eq:sample-covariance}
    \]
\end{itemize}

\subsubsection{Derivation of the Classifier}

We will start from the Bayes optimal classifier and derive the LDA classifier under the previous assumptions and using the mentioned estimates.

\begin{align*}
\hat{Y} &= \arg\max_{k=1}^K \, \mathbb{P}(Y = k \mid X = x) \\
&= \arg\max_{k=1}^K \, \frac{f_k(x) \, \hat{\pi}_k}{\sum_{\ell=1}^K f_\ell(x) \, \hat{\pi}_\ell} \\
&= \arg\max_{k=1}^K \, f_k(x) \hat{\pi}_k \\
&= \arg\max_{k=1}^K \, \left\{\frac{1}{(2\pi)^{p/2} |\hat{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k)\right) \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \, \left\{-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \, \left\{-\frac{1}{2}x^T \hat{\Sigma}^{-1} x + x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2}\hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \, \left\{x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2}\hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k\right\}
\end{align*}

We have then derived a set of functions that, given an unseen feature vector $x$, the classifier assigns it to the class $k$ that maximizes the respective function. These functions are known as the discriminant functions.

\begin{definition}
The discriminant function for class $k$ is:
\[
\delta_k(x) = x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log(\hat{\pi}_k)
\]
\end{definition}

\subsubsection{Numerical Implementation}

To evaluate the discriminant functions efficiently, we will compute and store the coefficients $c_k$ and $d_k$ for each class $k$ from the training data:
\begin{align*}
c_k &= \hat{\Sigma}^{-1} \hat{\mu}_k \\
d_k &= -\frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k
\end{align*}

Then the discriminant function simplifies to:
\[
\delta_k(x) = x^T c_k + d_k
\]

While we could explicitly compute the covariance matrix and its inverse, in practice it becomes computationally expensive for datasets with a large number of features, as well as numerically unstable, so a different algorithm will be used.

\begin{definition}
The class-centered data matrix of the $k$-th class is defined as the matrix $X^{(k)} \in \mathbb{R}^{N_{k} \times p}$ whose rows are the data points of the $k$-th class with the respective mean subtracted:

$$
X^{(k)} = \begin{bmatrix}
x_{1} - \mu_{k} \\
x_{2} - \mu_{k}  \\
\vdots \\  \\
x_{N_{k}} - \mu_{k}
\end{bmatrix}
$$
\end{definition}

\begin{definition}
The class-centered data matrix $X_c \in \mathbb{R}^{N \times p}$ is the concatenation of the class-centered data matrices for all classes:
\[
X_c = \begin{bmatrix}
X^{(1)} \\ X^{(2)} \\ \vdots \\ X^{(K)}
\end{bmatrix}
\]
\end{definition}

Note that now, we can write the sample covariance matrix defined in Equation ?? as:

\[
\hat{\Sigma} = \frac{1}{N - K} X_c^T X_c
\]

Let $X_c = U D V^T$ be the singular value decomposition of the class-centered data matrix $X_c$. Then, the sample covariance matrix can be written as:

\begin{align*}
\hat{\Sigma} &= \frac{1}{N - K} X_{c}^{T} X_{c}  \\
&= \frac{1}{N - K} VD^{T}U^{T}UDV^{T} \\
&= \frac{1}{N - K} VD^{2}V^{T} \\
\end{align*}

And the inverse of the sample covariance matrix as:

\begin{align*}
\hat{\Sigma}^{-1} &= \left( \frac{1}{N-K} VD^{2}V^{T} \right)^{-1} \\
& = VD^{-2}V^{T} (N-K)
\end{align*}

To reduce the number of multiplications, we define the intermediate coefficients $\alpha_k$ and $\beta_k$ for each class $k$:


\begin{align*}
\alpha_{k} & = V^{T} \mu_{k}  \\
\beta_{k} &= D^{-1}\alpha_{k}
\end{align*}

Now the $c_{k}$ coefficients simplify to

\begin{align*}
c_{k} &= \Sigma^{-1} \mu_{k} \\
&= VD^{-2}V^{T} (N-K) \mu_{k} \\
&= VD^{-2} \alpha_{k} (N-K) \\
\end{align*}

and the $d_{k}$ coefficients simplify to

\begin{align*}
d_{k} &= -\frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k} + \log \pi_{k} \\
&= -\frac{1}{2} \mu_{k}^{T} VD^{-2}V^{T} (N-K) \mu_{k} + \log \pi_{k} \\
&= -\frac{1}{2} \alpha_{k}^{T} D^{-2} \alpha_{k} (N-K) + \log \pi_{k}  \\
&= -\frac{1}{2} \beta_{k}^{T} \beta_{k} (N-K) + \log \pi_{k} \\
& = - \frac{N-K}{2} \| \beta_{k} \|^2_{2} + \log \pi_{k}
\end{align*}

The resulting algorithm for training the LDA classifier is summarized in Algorithm ??

\begin{algorithm}[htbp]
\caption{LDA Training Algorithm}
\begin{algorithmic}[1]
\REQUIRE Training data $\{(x_i, y_i)\}_{i=1}^N$, with $x_i \in \mathbb{R}^p$, $y_i \in \{1,\ldots,K\}$
\ENSURE Coefficients $\{ (c_k, d_k) \}_{k=1}^K$ for the discriminant functions

\STATE \textbf{Initialization:}
\begin{itemize}
    \item $N \gets$ total number of samples
    \item For each class $k$, initialize $N_k \gets 0$, $\hat{\mu}_k \gets 0$
\end{itemize}

\STATE \textbf{Compute Class Counts and Sums:}
\FOR{$i = 1$ to $N$}
    \STATE $k \gets y_i$
    \STATE $N_k \gets N_k + 1$
    \STATE $\hat{\mu}_k \gets \hat{\mu}_k + x_i$
\ENDFOR

\STATE \textbf{Compute Class Means and Priors:}
\FOR{$k = 1$ to $K$}
    \STATE $\hat{\mu}_k \gets \hat{\mu}_k / N_k$
    \STATE $\hat{\pi}_k \gets N_k / N$
\ENDFOR  % Corrected from \ENFOR

\STATE \textbf{Construct Centered Data Matrix:}
\STATE Initialize $X_c \in \mathbb{R}^{N \times p}$
\STATE $index \gets 1$
\FOR{$k = 1$ to $K$}
    \FOR{each $x_i$ such that $y_i = k$}
        \STATE $X_c(index, :) \gets x_i - \hat{\mu}_k$
        \STATE $index \gets index + 1$
    \ENDFOR
\ENDFOR

\STATE \textbf{Compute SVD of $X_c$:}
\STATE $[U, D, V^\top] \gets \text{svd}(X_c)$

\STATE \textbf{Compute Coefficients for Each Class:}
\FOR{$k = 1$ to $K$}
    \STATE $\alpha_k \gets V^\top \hat{\mu}_k$
    \STATE $\beta_k \gets D^{-1} \alpha_k$
    \STATE $c_k \gets (N - K) V D^{-2} \alpha_k$
    \STATE $d_k \gets -\dfrac{N - K}{2} \| \beta_k \|_2^2 + \ln \hat{\pi}_k$
\ENDFOR

\RETURN Coefficients $\{ (c_k, d_k) \}_{k=1}^K$
\end{algorithmic}
\end{algorithm}

\section{LDA for dimensionality reduction}

\subsection{$K-1$-dimensional representation}

At this point, we notice that in equation ?? in the decision rule derivation, we have something resembling a squared Euclidean distance, except for the presence of $\hat{\Sigma}^{-1}$. To simplify this, we can transform the data so that the covariance matrix becomes the identity, allowing us to directly use Euclidean distances.

In terms of the random vector $X$, the transformation $W$ we want to find must satisfy $\text{Cov}(WX) = I$. We can develop the left-hand side of this equation as:
\begin{align*}
\text{Cov}(WX) &= \mathbb{E}\left[ (WX - \mathbb{E}[WX])(WX - \mathbb{E}[WX])^T \right] \\
&= \mathbb{E}\left[ (WX - W\mu)(WX - W\mu)^T \right] \\
&= \mathbb{E}\left[ W(X - \mu)(X - \mu)^T W^T \right] \\
&= W \mathbb{E}\left[ (X - \mu)(X - \mu)^T \right] W^T \\
&= W \text{Cov}(X) W^T
\end{align*}

In LDA, we estimate $\text{Cov}(X)$ with $\hat{\Sigma}$, so we want to find $W$ such that $W \hat{\Sigma} W^T = I$. From the SVD computation we performed earlier, we know that $\hat{\Sigma} = \frac{1}{N-K}VD^2V^T$, where $V$ is orthogonal and $D$ is diagonal. Therefore,
\begin{align*}
I & = W \hat{\Sigma} W^T \\
& = W V (\frac{1}{N-K}D^2) V^T W^T
\end{align*}

We can then see that $W = \sqrt{N-K}D^{-1}V^T$ satisfies the equation. Effectively,

\begin{align*}
W \hat{\Sigma} W^T &= (\sqrt{N-K}D^{-1}V^T) \frac{1}{N-K}VD^2V^T (\sqrt{N-K}D^{-1}V^T)^T \\
&= (D^{-1}V^T) VD^2V^T (D^{-1}V^T)^T \\
&= D^{-1}V^T VD^2V^T V D^{-1} \\
&= D^{-1}D^2 D^{-1} \\
&= D^{-1}D^{1/2} D^{1/2} D^{-1} \\
& = I
\end{align*}


This is called a sphering or whitening transformation. We can then define:
\begin{itemize}
    \item The transformed sample: $x^* = \sqrt{N-K}D^{-1}V^Tx$
    \item The transformed means: $\mu_k^* = \sqrt{N-K}D^{-1}V^T\hat{\mu}_k$
\end{itemize}

Under this transformation, the decision criterion simplifies to:
\begin{align*}
\hat{Y} &= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(V D x^* - V D \mu_k^*)^T V^T D^{-2} V (V D x^* - V D \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(D x^* - D \mu_k^*)^T D^{-2} (D x^* - D \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x^* - \mu_k^*)^T (D D^{-2} D) (x^* - \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x^* - \mu_k^*)^T (x^* - \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}\|x^* - \mu_k^*\|_2^2 + \log \hat{\pi}_k\right\}
\end{align*}

\begin{definition}
The discriminant function for class $k$ in the transformed space is:
\[
\delta_k^*(x^*) = -\frac{1}{2}\|x^* - \mu_k^*\|_2^2 + \log \hat{\pi}_k
\]
\end{definition}

This transformation makes the decision rule more intuitive, effectively reducing the classification problem to one based on Euclidean distances to $K$ points in the transformed space, while maintaining the same decision boundaries. Notice that only the projection of $x$ in the subspace spanned by the transformed class means affects the decision. This suggests that LDA can be used as a dimensionality reduction technique, projecting the data into a lower-dimensional space while maximizing class separability.

%% theorem that the subspace that contains the class means is K-1 dimensional at most
\begin{theorem}
The subspace spanned by the tranformed class means $\mu_k^*$ is at most $K-1$ dimensional.
\[
\dim(\mathrm{span}(\mu_1^*, \ldots, \mu_K^*)) \leq K-1
\]
\end{theorem}

\begin{proof}
We assumed the data to be centered, therefore,
\begin{align*}
\mu &= 0 \\
\frac{1}{N} \sum_{k=1}^K N_k \mu_k &= 0 \\
\frac{1}{N} \sum_{k=1}^K N_k W\mu_k &= W0 \\
\frac{1}{N} \sum_{k=1}^K N_k \mu_k^* & = 0
\end{align*}

This means that the class means $\mu_k^*$ are linearly dependent, and therefore, the subspace spanned by them must have a dimension strictly less than $K$.
\end{proof}

In modern machine learning libraries such as \texttt{scikit-learn}, a de-sphering transformation is often applied at the end when using LDA for dimensionality reduction, so that the plotted data conserves the original covariance structure. In exchange, the decision rule is less intuitive in a lower-dimension plot.

\subsection{$L$-dimensional representation}
Often, $K-1$ is still too high a dimension for visualization or further processing. In these cases, we may want to project the data into an $L$-dimensional subspace, where $L \leq K-1$, while maximizing class separability. For this, we want to find the matrix $W \in \mathbb{R}^{p \times L}$ that projects the data into an $L$-dimensional subspace, maximizing the between-class variance while minimizing the within-class variance.

In order to quantify the within-class and between-class variance, we define the following matrices:

\begin{definition}
The within-class scatter matrix $S_W$ is defined as:
\[
S_W = \sum_{k=1}^K \sum_{i: y_i = k} (x_i - \mu_k)(x_i - \mu_k)^T
\]
\end{definition}

Note that $S_W$ can be expressed in terms of the class-centered data matrix $X_c$ or the sample covariance matrix $\hat{\Sigma}$ as:

\[
S_W = X_c^T X_c = (N - K) \hat{\Sigma}
\]

\begin{definition}
The between-class scatter matrix $S_B$ is defined as:
\[
S_B = \sum_{k=1}^K N_k (\mu_k - \mu)(\mu_k - \mu)^T
\]
\end{definition}

Under our assumption of centered data, $S_B$ simplifies to:

\[
S_B = \sum_{k=1}^K N_k \mu_k \mu_k^T
\]

The goal is to find the projection matrix $W$ that maximizes the ratio of the between-class scatter to the within-class scatter.

\subsubsection{$L=1$ case}
For $L=1$, the projection matrix $W$ is a vector $w \in \mathbb{R}^p$, and from equation ??, we see that maximizing the between-class scatter is equivalent to maximizing $w^T S_B w$, while minimizing the within-class scatter is equivalent to minimizing $w^T S_W w$. Unifying these two objectives, we define the Fisher criterion.

\begin{definition}
The Fisher criterion is defined as the ratio of the between-class scatter to the within-class scatter:
\[
J(w) = \frac{w^T S_B w}{w^T S_W w}
\]
\end{definition}


\begin{theorem}
The Fisher criterion is invariant to rescaling of the projection vector $w$.
\[
\forall \alpha \in \mathbb{R} \setminus \{0\}, \quad J(\alpha w) = J(w) \quad
\]
\end{theorem}

\begin{proof}
\begin{align*}
J(\alpha w) &= \frac{(\alpha w)^T S_B (\alpha w)}{(\alpha w)^T S_W (\alpha w)} \\
& = \frac{\alpha^2 w^T S_B w}{\alpha^2 w^T S_W w} \\
& = \frac{w^T S_B w}{w^T S_W w} \\
& = J(w)
\end{align*}
\end{proof}

Because of this invariance, we can impose the constraint $w^T S_W w = 1$ to simplify the optimization problem, which now reads:

\begin{align*}
& \underset{w \in \mathbb{R}^p}{\mathrm{maximize}} \quad w^T S_B w \\
& \text{subject to} \quad w^T S_W w = 1
\end{align*}

\begin{theorem}
The solution to the optimization problem ?? is given by the eigenvector corresponding to the largest eigenvalue of the matrix $S_W^{-1} S_B$.
\end{theorem}

\begin{proof}
The Lagrangian of the optimization problem is:
\[
\mathcal{L}(w, \lambda) = w^T S_B w - \lambda(w^T S_W w - 1)
\]

To take the derivative respect to $w$, we will use the following matrix calculus identity:
\[
\frac{\partial w^T A w}{\partial w} = 2Aw
\]

Differentiating the Lagrangian with respect to $w$ and setting it to zero, we get:
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w} &= 2S_B w - 2\lambda S_W w \\
0 &= S_B w - \lambda S_W w \\
S_B w &= \lambda S_W w \\
S_W^{-1} S_B w &= \lambda w
\end{align*}

And therefore the solution must be an eigenvector of $S_W^{-1} S_B$, associated to the eigenvalue $\lambda$. Replacing in the original expression,

\begin{align*}
J(w) &= \frac{w^T S_B w}{w^T S_W w} \\
&= \frac{w^T \lambda S_W w}{w^T S_W w} \\
& = \lambda
\end{align*}

Therefore, $w$ must be the eigenvector corresponding to the largest eigenvalue of $S_W^{-1} S_B$.

\end{proof}

\subsubsection{$L>1$ case}
For $L>1$, the projection matrix $W$ is a matrix $W \in \mathbb{R}^{p \times L}$, and $W^T S_B W$ and $W^T S_W W$ are now $L \times L$ matrices. Different generalizations of the Fisher criterion exist, most of which however have the same solution. One common generalization is the following:

\begin{definition}
The generalized Fisher criterion is defined as the trace of the ratio of the between-class scatter to the within-class scatter:

\[
J(W) = \text{tr}((W^T S_B W)^{-1} (W^T S_W W))
\]
\end{definition}

\begin{theorem}
The solution to the optimization problem ?? is given by the matrix $W$ whose columns are the eigenvectors corresponding to the $L$ largest eigenvalues of the matrix $S_W^{-1} S_B$.
\end{theorem}

The proof of this theorem is substantially more complex than the $L=1$ case, as the use of the derivative of the trace operator respect to a matrix is involved, making the equations more intricate. The structure of the proof can be found in ?? and the required matrix calculus identities in ??.

\end{document}
