\subsection{LDA: Aproximación al clasificador óptimo de Bayes}

En la práctica, obtener el clasificador óptimo de Bayes resulta generalmente imposible, ya que requiere conocer las probabilidades a posteriori $\mathbb{P}(Y = k \mid X = x)$. Estas probabilidades típicamente son desconocidas, pues dependen de la distribución subyacente de los datos, que rara vez está disponible en escenarios reales. Por tanto, necesitamos aproximar el clasificador de Bayes mediante suposiciones y estimaciones.

Para aproximar el clasificador de Bayes, emplearemos el teorema de Bayes. Introducimos la siguiente notación:
\begin{itemize}
    \item Sea $f_k(x)$ la densidad condicional de $X$ condicionada por la clase $Y = k$.
    \item Sea $\pi_k$ la probabilidad a priori de la clase $k$.
\end{itemize}

Mediante el teorema de Bayes, podemos expresar la probabilidad a posteriori $\mathbb{P}(Y = k \mid X = x)$ como:
\[
\mathbb{P}(Y = k \mid X = x) = \frac{f_k(x) \, \pi_k}{\sum_{\ell=1}^K f_\ell(x) \, \pi_\ell}
\]

Esta formulación nos permite aproximar las probabilidades a posteriori utilizando estimaciones de las densidades condicionales $f_k(x)$ y las probabilidades a priori $\pi_k$.

LDA realiza las siguientes suposiciones fundamentales:

\begin{enumerate}
    \item \textbf{Distribuciones gaussianas multivariadas}: Las densidades condicionales $f_k(x)$ siguen distribuciones gaussianas multivariadas con medias $\mu_k$ y matriz de covarianza común $\Sigma$:
    \[
    f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\right)
    \]
    \item \textbf{Matriz de covarianza común}: Todas las clases comparten la misma matriz de covarianza $\Sigma$.
\end{enumerate}

Los parámetros de la distribución y las probabilidades a priori pueden estimarse a partir de datos de entrenamiento. Con este propósito, definimos algunas notaciones:

\begin{itemize}
    \item Sea $N_k$ el número de observaciones en la clase $k$.
    \item Sea $N = \sum_{k=1}^K N_k$ el número total de observaciones.
    \item Sea $\{(x_i, g_i)\}_{i=1}^N$ los datos de entrenamiento, donde $x_i$ son los vectores de características y $g_i$ son las etiquetas de clase verdaderas.
\end{itemize}

Supondremos además que los datos están centrados,

\[
\hat{\mu} = \sum_{i=1}^N x_i = 0
\]

Y que el número de muestras supera al número de características, y este a su vez al número de clases,

\[
N > p > K
\]

Estimaremos los parámetros del modelo mediante:

\begin{itemize}
    \item \textbf{Probabilidades a Priori de Clase}:
    \[
    \hat{\pi}_k = \frac{N_k}{N}
    \]
    \item \textbf{Medias de Clase}:
    \[
    \hat{\mu}_k = \frac{1}{N_k} \sum_{i: g_i = k} x_i
    \]
    \item \textbf{Matriz de Covarianza}:
    \begin{equation}
    \hat{\Sigma} = \frac{1}{N - K} \sum_{k=1}^K \sum_{i: g_i = k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \label{eq:sample-covariance}
    \end{equation}
\end{itemize}

Asumiremos que la matriz de covarianza muestral es invertible.

\subsubsection{Derivación del clasificador}

Partiendo del clasificador óptimo de Bayes, derivaremos el clasificador LDA bajo las suposiciones anteriores y utilizando las estimaciones mencionadas.

\begin{align}
\hat{Y} &= \arg\max_{k=1}^K \, \mathbb{P}(Y = k \mid X = x) \nonumber \\
&= \arg\max_{k=1}^K \, \frac{f_k(x) \, \hat{\pi}_k}{\sum_{\ell=1}^K f_\ell(x) \, \hat{\pi}_\ell} \nonumber \\
&= \arg\max_{k=1}^K \, f_k(x) \hat{\pi}_k \nonumber \\
&= \arg\max_{k=1}^K \, \left\{\frac{1}{(2\pi)^{p/2} |\hat{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k)\right) \hat{\pi}_k\right\} \nonumber \\
&= \arg\max_{k=1}^K \, \left\{-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k) + \log \hat{\pi}_k\right\} \label{eq:mahalanobis} \\
&= \arg\max_{k=1}^K \, \left\{-\frac{1}{2}x^T \hat{\Sigma}^{-1} x + x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2}\hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k\right\} \nonumber \\
&= \arg\max_{k=1}^K \, \left\{x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2}\hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k\right\} \nonumber
\end{align}

Hemos derivado un conjunto de funciones que, dado un vector de características no visto $x$, el clasificador lo asigna a la clase $k$ que maximiza la respectiva función. Estas funciones se conocen como funciones discriminantes.

\begin{definition}
La función discriminante para la clase $k$ es:
\[
\delta_k(x) = x^T \hat{\Sigma}^{-1} \hat{\mu}_k - \frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log(\hat{\pi}_k)
\]
\end{definition}

\subsubsection{Implementación numérica}

Para evaluar las funciones discriminantes eficientemente, calcularemos y almacenaremos los coeficientes $c_k$ y $d_k$ para cada clase $k$ a partir de los datos de entrenamiento:
\begin{align*}
c_k &= \hat{\Sigma}^{-1} \hat{\mu}_k \\
d_k &= -\frac{1}{2} \hat{\mu}_k^T \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k
\end{align*}

Luego, la función discriminante se simplifica a:
\[
\delta_k(x) = x^T c_k + d_k
\]

Aunque podríamos calcular explícitamente la matriz de covarianza y su inversa, en la práctica esto se vuelve computacionalmente costoso para conjuntos de datos con un gran número de características, así como numéricamente inestable, por lo que se utilizará un algoritmo diferente.

\begin{definition}
La matriz de datos centrados por clase de la clase $k$ se define como la matriz $X^{(k)} \in \mathbb{R}^{N_{k} \times p}$ cuyos filas son los puntos de datos de la clase $k$ con la respectiva media restada:

$$
X^{(k)} = \begin{bmatrix}
x_{1} - \mu_{k} \\
x_{2} - \mu_{k}  \\
\vdots \\  \\
x_{N_{k}} - \mu_{k}
\end{bmatrix}
$$
\end{definition}

\begin{definition}
La matriz de datos centrados por clase $X_c \in \mathbb{R}^{N \times p}$ es la concatenación de las matrices de datos centrados por clase para todas las clases:
\[
X_c = \begin{bmatrix}
X^{(1)} \\ X^{(2)} \\ \vdots \\ X^{(K)}
\end{bmatrix}
\]
\end{definition}

Asumiremos para simplificar que $X_c$ tiene rango completo.

Observemos que ahora podemos escribir la matriz de covarianza muestral definida en la Ecuación \ref{eq:sample-covariance} como:
\[
\hat{\Sigma} = \frac{1}{N - K} X_c^T X_c
\]

Sea $X_c = U D V^T$ la descomposición en valores singulares (SVD) de la matriz de datos centrados por clase $X_c$. Entonces, la matriz de covarianza muestral se puede escribir como:
\begin{align*}
\hat{\Sigma} &= \frac{1}{N - K} X_{c}^{T} X_{c}  \\
&= \frac{1}{N - K} VD^{T}U^{T}UDV^{T} \\
&= \frac{1}{N - K} VD^{2}V^{T} \\
\end{align*}

Y la inversa de la matriz de covarianza muestral como:
\begin{align*}
\hat{\Sigma}^{-1} &= \left( \frac{1}{N-K} VD^{2}V^{T} \right)^{-1} \\
& = VD^{-2}V^{T} (N-K)
\end{align*}

Para reducir el número de multiplicaciones, definimos los coeficientes intermedios $\alpha_k$ y $\beta_k$ para cada clase $k$:

\begin{align*}
\alpha_{k} & = V^{T} \mu_{k}  \\
\beta_{k} &= D^{-1}\alpha_{k}
\end{align*}

Ahora, los coeficientes $c_{k}$ se simplifican a

\begin{align*}
c_{k} &= \Sigma^{-1} \mu_{k} \\
&= VD^{-2}V^{T} (N-K) \mu_{k} \\
&= VD^{-2} \alpha_{k} (N-K) \\
\end{align*}

y los coeficientes $d_{k}$ se simplifican a

\begin{align*}
d_{k} &= -\frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k} + \log \pi_{k} \\
&= -\frac{1}{2} \mu_{k}^{T} VD^{-2}V^{T} (N-K) \mu_{k} + \log \pi_{k} \\
&= -\frac{1}{2} \alpha_{k}^{T} D^{-2} \alpha_{k} (N-K) + \log \pi_{k}  \\
&= -\frac{1}{2} \beta_{k}^{T} \beta_{k} (N-K) + \log \pi_{k} \\
& = - \frac{N-K}{2} \| \beta_{k} \|^2_{2} + \log \pi_{k}
\end{align*}

El algoritmo resultante para entrenar el clasificador LDA se resume en el Algoritmo \ref{alg:lda-training}.

\begin{algorithm}[htbp]
\caption{Algoritmo de Entrenamiento de LDA} \label{alg:lda-training}
\begin{algorithmic}[1]
\REQUIRE Datos de entrenamiento $\{(x_i, y_i)\}_{i=1}^N$, con $x_i \in \mathbb{R}^p$, $y_i \in \{1,\ldots,K\}$
\ENSURE Coeficientes $\{ (c_k, d_k) \}_{k=1}^K$ para las funciones discriminantes

\STATE \textbf{Inicialización:}
\begin{itemize}
    \item $N \gets$ número total de muestras
    \item Para cada clase $k$, inicializar $N_k \gets 0$, $\hat{\mu}_k \gets 0$
\end{itemize}

\STATE \textbf{Calcular Conteos de Clases y Sumas:}
\FOR{$i = 1$ to $N$}
    \STATE $k \gets y_i$
    \STATE $N_k \gets N_k + 1$
    \STATE $\hat{\mu}_k \gets \hat{\mu}_k + x_i$
\ENDFOR

\STATE \textbf{Calcular Medias de Clases y Priors:}
\FOR{$k = 1$ to $K$}
    \STATE $\hat{\mu}_k \gets \hat{\mu}_k / N_k$
    \STATE $\hat{\pi}_k \gets N_k / N$
\ENDFOR

\STATE \textbf{Construir la Matriz de Datos Centrados:}
\STATE Inicializar $X_c \in \mathbb{R}^{N \times p}$
\STATE $j \gets 1$
\FOR{$k = 1$ to $K$}
    \FOR{cada $x_i$ tal que $y_i = k$}
        \STATE $X_c(j, :) \gets x_i - \hat{\mu}_k$
        \STATE $j \gets j + 1$
    \ENDFOR
\ENDFOR

\STATE \textbf{Calcular SVD de $X_c$:}
\STATE $[U, D, V^\top] \gets \text{svd}(X_c)$

\STATE \textbf{Calcular Coeficientes para Cada Clase:}
\FOR{$k = 1$ to $K$}
    \STATE $\alpha_k \gets V^\top \hat{\mu}_k$
    \STATE $\beta_k \gets D^{-1} \alpha_k$
    \STATE $c_k \gets (N - K) V D^{-2} \alpha_k$
    \STATE $d_k \gets -\dfrac{N - K}{2} \| \beta_k \|_2^2 + \ln \hat{\pi}_k$
\ENDFOR

\RETURN Coeficientes $\{ (c_k, d_k) \}_{k=1}^K$
\end{algorithmic}
\end{algorithm}
