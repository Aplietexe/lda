\section{LDA para reducción de dimensionalidad}

\subsection{Representación $K-1$ dimensional}

Observemos que en la ecuación \ref{eq:mahalanobis}, en la derivación del clasificador, tenemos una expresión que se asemeja a una distancia euclidiana al cuadrado, salvo por la presencia de $\hat{\Sigma}^{-1}$. Para simplificar esto, podemos transformar los datos de modo que la matriz de covarianza se convierta en la identidad, lo que nos permitirá usar directamente distancias euclidianas.

En términos del vector aleatorio $X$, la transformación $W$ que queremos encontrar debe satisfacer $\text{Cov}(WX) = I$. Podemos desarrollar el lado izquierdo de esta ecuación como:

\begin{align}
\text{Cov}(WX) &= \mathbb{E}\left[ (WX - \mathbb{E}[WX])(WX - \mathbb{E}[WX])^T \right] \nonumber \\
&= \mathbb{E}\left[ (WX - W\mu)(WX - W\mu)^T \right] \nonumber \\
&= \mathbb{E}\left[ W(X - \mu)(X - \mu)^T W^T \right] \nonumber \\
&= W \mathbb{E}\left[ (X - \mu)(X - \mu)^T \right] W^T \nonumber \\
&= W \text{Cov}(X) W^T \label{eq:covariance-transformation}
\end{align}

En LDA, estimamos $\text{Cov}(X)$ con $\hat{\Sigma}$, por lo que queremos encontrar $W$ tal que $W \hat{\Sigma} W^T = I$. A partir de la computación de la SVD que realizamos anteriormente, sabemos que $\hat{\Sigma} = \frac{1}{N-K}VD^2V^T$, donde $V$ es ortogonal y $D$ es diagonal. Por lo tanto,

\begin{align*}
I & = W \hat{\Sigma} W^T \\
& = W V \left(\frac{1}{N-K}D^2\right) V^T W^T
\end{align*}

Entonces, podemos ver que $W = \sqrt{N-K}D^{-1}V^T$ satisface la ecuación. Efectivamente,

\begin{align*}
W \hat{\Sigma} W^T &= (\sqrt{N-K}D^{-1}V^T) \frac{1}{N-K}VD^2V^T (\sqrt{N-K}D^{-1}V^T)^T \\
&= (D^{-1}V^T) VD^2V^T (D^{-1}V^T)^T \\
&= D^{-1}V^T V D^2 V^T V D^{-1} \\
&= D^{-1}D^2 D^{-1} \\
&= D^{-1}D^{1/2} D^{1/2} D^{-1} \\
& = I
\end{align*}

Esto se llama una transformación de sphering o whitening. Entonces podemos definir:
\begin{itemize}
    \item La muestra transformada: $x^* = \sqrt{N-K}D^{-1}V^Tx$
    \item Las medias transformadas: $\mu_k^* = \sqrt{N-K}D^{-1}V^T\hat{\mu}_k$
\end{itemize}

Bajo esta transformación, el criterio de decisión se simplifica a:
\begin{align*}
\hat{Y} &= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x - \hat{\mu}_k)^T \hat{\Sigma}^{-1} (x - \hat{\mu}_k) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(V D x^* - V D \mu_k^*)^T V^T D^{-2} V (V D x^* - V D \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(D x^* - D \mu_k^*)^T D^{-2} (D x^* - D \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x^* - \mu_k^*)^T (D D^{-2} D) (x^* - \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}(x^* - \mu_k^*)^T (x^* - \mu_k^*) + \log \hat{\pi}_k\right\} \\
&= \arg\max_{k=1}^K \left\{-\frac{1}{2}\|x^* - \mu_k^*\|_2^2 + \log \hat{\pi}_k\right\}
\end{align*}

\begin{definition}
La función discriminante para la clase $k$ en el espacio transformado es:
\[
\delta_k^*(x^*) = -\frac{1}{2}\|x^* - \mu_k^*\|_2^2 + \log \hat{\pi}_k
\]
\end{definition}

Esta transformación hace que la regla de decisión sea más intuitiva, reduciendo efectivamente el problema de clasificación a uno basado en distancias euclidianas a $K$ puntos en el espacio transformado, mientras que mantiene las mismas fronteras de decisión. Observemos que solo la proyección de $x$ en el subespacio abarcado por las medias transformadas de clase afecta la decisión. Esto sugiere que LDA puede usarse como una técnica de reducción de dimensionalidad, proyectando los datos en un espacio de menor dimensión mientras se maximiza la separabilidad de las clases.

\begin{theorem}
El subespacio generado por las medias transformadas $\mu_k^*$ es de dimensión menor o igual a $K-1$.
\[
\dim(\mathrm{span}(\mu_1^*, \ldots, \mu_K^*)) \leq K-1
\]
\end{theorem}

\begin{proof}
Asumimos que los datos están centrados, por lo tanto:
\begin{align*}
\mu &= 0 \\
\frac{1}{N} \sum_{k=1}^K N_k \mu_k &= 0 \\
\frac{1}{N} \sum_{k=1}^K N_k W\mu_k &= W0 \\
\frac{1}{N} \sum_{k=1}^K N_k \mu_k^* & = 0
\end{align*}

Esto significa que las medias $\mu_k^*$ son linealmente dependientes, y por lo tanto, el subespacio que abarcan debe tener una dimensión estrictamente menor que $K$.
\end{proof}

En bibliotecas modernas de aprendizaje automático como \texttt{scikit-learn}, a menudo se aplica una transformación de de-sphering como paso final cuando se usa LDA para reducción de dimensionalidad, de modo que los datos conserven la estructura de covarianza original. A cambio, la regla de decisión es menos intuitiva si se realiza un gráfico en el espacio transformado.

\subsection{Representación $L$-dimensional}

Con frecuencia, $K-1$ dimensiones siguen siendo demasiadas para visualización o procesamiento adicional. En estos casos, podemos desear proyectar los datos en un subespacio de $L$ dimensiones, donde $L \leq K-1$, mientras maximizamos la separabilidad entre clases. Para ello, buscamos la matriz $W \in \mathbb{R}^{p \times L}$ que proyecte los datos en un subespacio de $L$ dimensiones, maximizando la varianza entre clases mientras minimiza la varianza dentro de las clases.

Para cuantificar la varianza dentro de clases y entre clases, definimos las siguientes matrices:

\begin{definition}
La matriz de dispersión dentro de clases $S_W$ se define como:
\[
S_W = \sum_{k=1}^K \sum_{i: y_i = k} (x_i - \mu_k)(x_i - \mu_k)^T
\]
\end{definition}

Observemos que $S_W$ puede expresarse en términos de la matriz de datos centrados por clase $X_c$ o de la matriz de covarianza muestral $\hat{\Sigma}$ como:
\[
S_W = X_c^T X_c = (N - K) \hat{\Sigma}
\]

\begin{definition}
La matriz de dispersión entre clases $S_B$ se define como:
\[
S_B = \sum_{k=1}^K N_k (\mu_k - \mu)(\mu_k - \mu)^T
\]
\end{definition}

Bajo nuestra suposición de datos centrados, $S_B$ se simplifica a:
\[
S_B = \sum_{k=1}^K N_k \mu_k \mu_k^T
\]

El objetivo es encontrar la matriz de proyección $W$ que maximice la razón de la dispersión entre clases a la dispersión dentro de clases.

\subsubsection{Caso $L=1$}

Para $L=1$, la matriz de proyección $W$ es un vector $w \in \mathbb{R}^p$, y a partir de la ecuación \ref{eq:covariance-transformation}, vemos que maximizar la dispersión entre clases equivale a maximizar $w^T S_B w$, mientras que minimizar la dispersión dentro de clases equivale a minimizar $w^T S_W w$. Unificando estos dos objetivos, definimos el criterio de Fisher.

\begin{definition}
El criterio de Fisher se define como la razón de la dispersión entre clases a la dispersión dentro de clases:
\[
J(w) = \frac{w^T S_B w}{w^T S_W w}
\]
\end{definition}

\begin{theorem}
El criterio de Fisher es invariante bajo un escalado del vector de proyección $w$.
\[
\forall \alpha \in \mathbb{R} \setminus \{0\}, \quad J(\alpha w) = J(w) \quad
\]
\end{theorem}

\begin{proof}
\begin{align*}
J(\alpha w) &= \frac{(\alpha w)^T S_B (\alpha w)}{(\alpha w)^T S_W (\alpha w)} \\
& = \frac{\alpha^2 w^T S_B w}{\alpha^2 w^T S_W w} \\
& = \frac{w^T S_B w}{w^T S_W w} \\
& = J(w)
\end{align*}
\end{proof}

Debido a esta invariancia, podemos imponer la restricción $w^T S_W w = 1$ para simplificar el problema de optimización, que ahora se escribe como:

\begin{align} \label{eq:single_dimension_optimization}
& \underset{w \in \mathbb{R}^p}{\mathrm{maximize}} \quad w^T S_B w \\
& \text{subject to} \quad w^T S_W w = 1 \nonumber
\end{align}

\begin{theorem}
La solución al problema de optimización \ref{eq:single_dimension_optimization} está dada por el autovector correspondiente al mayor autovalor de la matriz $S_W^{-1} S_B$.
\end{theorem}

\begin{proof}
El Lagrangiano del problema de optimización es:
\[
\mathcal{L}(w, \lambda) = w^T S_B w - \lambda(w^T S_W w - 1)
\]

Para derivar con respecto a $w$, utilizamos la siguiente identidad del cálculo matricial:
\[
\frac{\partial w^T A w}{\partial w} = 2Aw
\]

Derivando el Lagrangiano con respecto a $w$ e igualando a cero, obtenemos:
\begin{align*}
\frac{\partial \mathcal{L}}{\partial w} &= 2S_B w - 2\lambda S_W w \\
0 &= S_B w - \lambda S_W w \\
S_B w &= \lambda S_W w \\
S_W^{-1} S_B w &= \lambda w
\end{align*}

Por lo tanto, la solución debe ser un autovector de $S_W^{-1} S_B$, asociado al autovalor $\lambda$. Reemplazando en la expresión original,

\begin{align*}
J(w) &= \frac{w^T S_B w}{w^T S_W w} \\
&= \frac{w^T \lambda S_W w}{w^T S_W w} \\
& = \lambda
\end{align*}

Por lo tanto, $w$ debe ser el autovector correspondiente al mayor autovalor de $S_W^{-1} S_B$.
\end{proof}

\subsubsection{Caso $L>1$}

Para $L>1$, la matriz de proyección $W$ es una matriz $W \in \mathbb{R}^{p \times L}$, y $W^T S_B W$ y $W^T S_W W$ son ahora matrices $L \times L$. Existen diferentes generalizaciones del criterio de Fisher, la mayoría de las cuales, sin embargo, tienen la misma solución. Una generalización común es la siguiente:

\begin{definition}
El criterio de Fisher generalizado se define como:
\[
J(W) = \text{tr}((W^T S_B W)^{-1} (W^T S_W W))
\]
\end{definition}

\begin{theorem}
El máximo del criterio de Fisher generalizado $J(W)$ es alcanzado por la matriz $W$ cuyas columnas son los autovectores correspondientes a los $L$ mayores autovalores de la matriz $S_W^{-1} S_B$.
\end{theorem}

La demostración de este teorema es sustancialmente más compleja que el caso $L=1$, ya que el uso de la derivada del operador traza respecto a una matriz está involucrado, haciendo las ecuaciones más intrincadas. La estructura de la prueba puede encontrarse en \cite{fukunaga1990introduction}.
